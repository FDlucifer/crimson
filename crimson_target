#!/bin/bash
# TO DO:
# Make ffuf silent and add jq parsing.
# Make better printer
### FUNCTIONS
#
# 1. FULL RANGE PORT SCAN && NSE ON OPENED PORTS
# 2. VULN SCANS
# 3. SPIDER THE DOMAIN
# 4. DIRECTORY BRUTEFORCE
# 5. GATHER SOURCE CODE OF SCRAPED / BRUTEFORCED URLS
# 6. EXTRACT NEW PATHS, API KEYS, ENDPOINTS FROM GATHERED SOURCE CODE
# 7. MERGE PATHS WITH DOMAIN AND PROBE FOR NEW ENDPOINTS
# 8. SEND ALL FINDINGS WITHOUT 400/404 STATUS CODE TO BURP PROXY
# 9. PREPARE params.txt FOR EXPLOIT MODULE
# 10. PREPARE dirs.txt FOR EXPLOIT MODULE
# 11. CHECK WAF
# 12. CHECK BACKUP FILES
#
### LISTS:
#
# 1) recon.txt
# 2) urls.txt
# 3) status_params.txt
# 4) zile.txt
# 5) status_ffuf.txt ***** MAKE SURE U CHECKED IT AFTER BRUTE *****
# 6) status_new_endpoints.txt
# 7) ffuf.txt
# 8) status_dir.txt
# 9) exp/params.txt
# 10) exp/dirs.txt
# 11) backups.txt
### WORKFLOW
# 0. Start Burp
#	- Create new project - www.example.domain.com
#	- Turn off interception
# 1. Start the script
# 2. Check the output listed above (LISTS)
# 3. Manually browse the application, click on all functionalities
# 4. Copy whole target scope from Burp after manually browsing the target
# 5. Paste it to exp/all.txt and run crimson_exploit
###
echo -e "\033[0;31m
 ██████╗██████╗ ██╗███╗   ███╗███████╗ ██████╗ ███╗   ██╗     ████████╗ █████╗ ██████╗  ██████╗ ███████╗████████╗
██╔════╝██╔══██╗██║████╗ ████║██╔════╝██╔═══██╗████╗  ██║     ╚══██╔══╝██╔══██╗██╔══██╗██╔════╝ ██╔════╝╚══██╔══╝
██║     ██████╔╝██║██╔████╔██║███████╗██║   ██║██╔██╗ ██║        ██║   ███████║██████╔╝██║  ███╗█████╗     ██║   
██║     ██╔══██╗██║██║╚██╔╝██║╚════██║██║   ██║██║╚██╗██║        ██║   ██╔══██║██╔══██╗██║   ██║██╔══╝     ██║   
╚██████╗██║  ██║██║██║ ╚═╝ ██║███████║╚██████╔╝██║ ╚████║███████╗██║   ██║  ██║██║  ██║╚██████╔╝███████╗   ██║   
 ╚═════╝╚═╝  ╚═╝╚═╝╚═╝     ╚═╝╚══════╝ ╚═════╝ ╚═╝  ╚═══╝╚══════╝╚═╝   ╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝ ╚══════╝   ╚═╝   
                                                                                                                 
\033[0m"

while getopts "c:d:" OPTION; do
    case $OPTION in
    c)
        cookie=$OPTARG
        ;;
    d)
        domain=$OPTARG
        ;;
    *)
        echo "Incorrect options provided"
        exit 1
        ;;
    esac
done

if [ -z $domain ]
then
        echo "Usage: crimson_target -d \"domain_name\" -c \"Cookie: auth=cookie\""
        exit  1
else

### PREPARE DIRECTORIES AND VARIABLES
echo -e "\033[0;31m [+]\033[0m PREPARING DIRECTORIES AND VARIABLES"
export domain=$domain
export DOMAIN=`tldextract $domain | cut -d " " -f 2-3 | sed "s/\ /\./"`
export TARGET=`dig +short $domain`
mkdir $HOME/bounty/$DOMAIN/$domain/temp -p
mkdir $HOME/bounty/$DOMAIN/$domain/exp
mkdir $HOME/bounty/$DOMAIN/$domain/all_source_code
cd $HOME/bounty/$DOMAIN/$domain   

if [[ -z ${cookie} ]];
then
	cookie="Cookie: _ga=23489yfdshbfji324987sfdnik4327fdnskdfh834;";
else
	cookie=$cookie;
fi

### RESOLVE IP AND SCAN OPENED PORTS > recon.txt
echo "[NMAP] ---------------------------------------------------------" | tee -a recon.txt
echo TARGET: $TARGET | sed 's/\ /\n/g' | tee -a recon.txt
ports=$(nmap -p- --min-rate=1000 -T4 $TARGET | grep ^[0-9] | cut -d '/' -f 1 | sort -u | tr '\n' ',' | sed s/,$//)
echo PORTS : $ports | tee -a recon.txt
nmap -A -p $ports $TARGET -oG exp/nmap.gnmap -oN nmap.txt
cat nmap.txt >> recon.txt
rm nmap.txt

### GET THE CONTENT OF SITEMAP IF EXISTS >> recon.txt
echo "[SITEMAP 80] ---------------------------------------------------------" | tee -a recon.txt
$HOME/tools/sitemap-urls/sitemap-urls.sh http://$domain | tee -a recon.txt
echo "[SITEMAP 443] ---------------------------------------------------------" | tee -a recon.txt
$HOME/tools/sitemap-urls/sitemap-urls.sh https://$domain | tee -a recon.txt

### GET THE CONTENT OF ROBOTS IF EXISTS >> recon.txt
echo "[ROBOTS 80] ---------------------------------------------------------" | tee -a recon.txt
curl -O http://vpn.scopely.com/robots.txt
cat robots.txt >> recon.txt
echo "[ROBOTS 443] ---------------------------------------------------------" | tee -a recon.txt
curl -k -O https://vpn.scopely.com/robots.txt
cat robots.txt >> recon.txt
rm robots.txt

### CHECKING WAF >> recon.txt
echo "[WAFW00F] ---------------------------------------------------------" | tee -a recon.txt
wafw00f $domain | tail -n +16 | tee -a recon.txt

### IDENTIFY TECHNOLOGY 1 >> recon.txt
echo "[WEBTECH] ---------------------------------------------------------" | tee -a recon.txt
webtech -u http://$domain | tee -a recon.txt
webtech -u https://$domain | tee -a recon.txt

### IDENTIFY TECHNOLOGY 2 >> recon.txt
echo "[WHATWEB] ---------------------------------------------------------" | tee -a recon.txt
$HOME/tools/WhatWeb/whatweb -a 3 $domain -H $cookie | tee -a recon.txt

### VULN SCAN 1 >> recon.txt
# Perform a brute-force attack to uncover known and potentially dangerous scripts on the web server.
nl=$'\n'
cr=$'\r'
echo "[NITKO 443] ---------------------------------------------------------" | tee -a recon.txt
$HOME/tools/nikto/program/nikto.pl -host https://$domain -useragent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36${cr}${nl}$cookie" --maxtime 1500 | tee -a recon.txt
echo "[NIKTO 80] ----------------------------------------------------------" | tee -a recon.txt
$HOME/tools/nikto/program/nikto.pl -host http://$domain -useragent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36${cr}${nl}$cookie" --maxtime 1500 | tee -a recon.txt

### VULN SCAN 2 >> recon.txt
# Looking for: 
# 	cookie flags
#	crlf vuln
#	csp
#	file handling vulns
#	bypass weak htaccess conf
#	header security
#	methods 
#	shellshock vuln
echo "[WAPITI 80] ----------------------------------------------------------" | tee -a recon.txt
wapiti --no-bugreport -u http://$domain -o wapiti -f txt -H $cookie -m "cookieflags,crlf,csp,csrf,file,htaccess,http_headers,methods,shellshock"
cat wapiti | tail -n+8 >> recon.txt
rm wapiti
echo "[WAPITI 443] ---------------------------------------------------------" | tee -a recon.txt
wapiti --no-bugreport --verify-ssl 0 -u https://$domain -o wapiti -f txt -H $cookie -m "2cookieflags,crlf,csp,csrf,file,htaccess,http_headers,methods,shellshock"
cat wapiti | tail -n+8 >> recon.txt
rm wapiti

### SPIDER 1 > urls.txt
echo -e "\033[0;31m [+]\033[0m START SPIDERS"
gospider -q -r -w -a --sitemap -c 10 -s  https://$domain -H $cookie >> urls.txt

### SPIDER 2 >> urls.txt
python3 $HOME/tools/ParamSpider/paramspider.py -d $domain --output ./paramspider.txt --level high > /dev/null 2>&1
cat urls.txt ../paramspider.txt | grep http | sort -u | grep $domain >> urls.txt

### SPIDER 3 >> urls.txt
get-all-urls $domain >> urls.txt

### SPIDER 4 >> urls.txt
waybackurls $domain >> urls.txt

### SPIDER 5 >> urls.txt 
hakrawler -url $domain -depth 4 -linkfinder -insecure -wayback -usewayback -urls -subs -sitemap -robots -plain -js -headers $cookie >> urls.txt

### SPIDER 6 >> urls.txt
galer -u http://$domain -s >> urls.txt

### MERGE SPIDERS AND DELETE DUPLICATES >> urls.txt
echo -e "\033[0;31m [+]\033[0m MERGING SPIDERS RESULTS"
cat ../urls.txt | grep $domain | anew urls.txt 
cat urls.txt | qsreplace -a > temp1.txt
mv temp1.txt urls.txt
rm temp1.txt

### CHECK STATUS OF URLS WITH QUERIES && PROXY TO BURP > status_params.txt
echo -e "\033[0;31m [+]\033[0m PROXING QUERIES TO BURP"
cat urls.txt | grep "?" > temp/temp_params.txt
wfuzz -f status_params.txt,raw -L -Z -z file,temp/temp_params.txt -z file,$HOME/tools/CRIMSON/words/blank -p 127.0.0.1:8080 -H $cookie FUZZFUZ2Z > /dev/null 2>&1

### GET NEW ENDPOINTS FROM SPIDERS AND ADD THEM TO WORDLIST FOR DIRECOTRY BRUTEFORCING > custom_dir.txt
echo -e "\033[0;31m [+]\033[0m GATHERING NEW PATHS FROM SPIDERS RESULTS"
cat urls.txt | unfurl paths > temp1.txt
sort -u temp1.txt $HOME/tools/CRIMSON/words/dir > $HOME/tools/CRIMSON/words/custom_dir.txt
rm temp1.txt

### DIRECTORY BRUTEFORCING > status_ffuf.txt
echo -e "\033[0;31m [+]\033[0m STARTING DIRECTORY BRUTEFORCING"
# KARMAZ -  change to JSON and add jq parsing to get urls - remove strange filter. 
# ffuf -w $HOME/words/custom_dir.txt -u https://$domain/FUZZ -mc all -fc 400 -H $cookie -o temp1.txt > /dev/null 2>&1
ffuf -w $HOME/words/custom_dir.txt -u https://$domain/FUZZ -mc all -fc 400 -H $cookie | tee temp1.txt
cat temp1.txt |sed -r "s/\x1B\[([0-9]{1,2}(;[0-9]{1,2})?)?[m|K]//g" | sed "s/.//" > status_ffuf.txt
rm temp1.txt

### REMOVE TRASH RESPONSES FROM PREVIOUS SCAN > ffuf.txt
echo -e "\033[0;31m [+]\033[0m REMOVING TRASH RESPONSES FROM status_ffuf.txt"
python $HOME/tools/CRIMSON/scripts/clever_ffuf.py
sort -u temp_ffuf.txt | cut -d " " -f 1 > ffuf.txt
rm temp_ffuf.txt

### GATHER ALL SOURCE CODE FROM ffuf.txt AND STORE IT IN DIRECTORY > all_source_code/
echo -e "\033[0;31m [+]\033[0m GATHERING SOURCE CODE OF BRUTE-FORCED SITES"
for url in $(cat ffuf.txt); do echo $url && curl -k -s $url -H $cookie -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36" > all_source_code/`echo $url | sed "s/^https:\/\///" | sed "s/^http:\/\///" | sed "s/\//_/g"` ;done

### GET LINKS TO JS FILES AND PREPARE IT FOR EXPLOIT MODULE > exp/jsfiles.txt
echo -e "\033[0;31m [+]\033[0m GATHERING .js LINKS"
cat urls.txt ffuf.txt | getJS --complete --nocolors -H $cookie | grep "^http" >> exp/jsfiles.txt

### GET LINKS TO JS FILES FROM urls.txt > exp/jsfiles.txt
cat urls.txt | grep "\.js$" | grep "^http" | anew exp/jsfiles.txt

### CHECK FOR LIVE JS LINKS
echo -e "\033[0;31m [+]\033[0m CHECKING FOR LIVE .js LINKS"
httpx -silent -l exp/jsfiles.txt -H $cookie >> exp/jsfiles.txt

### GATHER SOURCE CODE FROM JS LINKS AND STORE IT IN >> all_source_code/
echo -e "\033[0;31m [+]\033[0m GATHERING SOURCE CODE FROM LIVE .js LINKS"
for url in $(cat exp/jsfiles.txt); do echo $url && curl -k -s $url -H $cookie -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36" > all_source_code/`echo $url | sed "s/^https:\/\///" | sed "s/^http:\/\///" | sed "s/\//_/g"` ;done

### DIG API KEYS / ENDPOINTS ETC. > zile.txt
echo -e "\033[0;31m [+]\033[0m GATHERING API KEYS && NEW PATHS"
cd all_source_code
python3 $HOME/tools/zile/zile.py --file >> ../temp/temp_zile.txt
cd ..
awk '!seen[$0]++' temp/temp_zile.txt | grep -v "[+]" > zile.txt

### GET ENDPOINTS FROM ZILE > extracted.txt
cat zile.txt | cut -d " " -f 2 | sed "s/^..//g" | sed "s/..$//g" | sed "s/\"//g" | unfurl path | sed "s/'$//" > temp/temp_zile_endpoints.txt
awk '!seen[$0]++' temp/temp_zile_endpoints.txt >> extracted.txt

### MUTATE URLS && CHECK IF THERE ARE NO DUPLICATES WITH ffuf.txt > new_endpoints.txt
for line in $(cat extracted.txt); do echo http://$domain$line >> temp/temp_new.txt; done
for line in $(cat extracted.txt); do echo https://$domain$line >> temp/temp_new.txt; done
rm extracted.txt
awk '!seen[$0]++' temp/temp_new.txt > temp/temp_new_endpoints.txt
sort ffuf.txt temp/temp_new_endpoints.txt | uniq -d > temp/temp_duplicates.txt
grep -v -x -f temp/temp_duplicates.txt temp/temp_new_endpoints.txt > new_endpoints.txt

### CHECK STATUS OF NEW URLS > status_new_endpoints.txt
echo -e "\033[0;31m [+]\033[0m CHECKING STATUS CODE OF NEW PATHS"
wfuzz -f status_new_endpoints.txt,raw -L -Z -z file,new_endpoints.txt -z file,$HOME/tools/CRIMSON/words/blank -H $cookie FUZZFUZ2Z > /dev/null 2>&1
rm new_endpoints.txt

### REMOVE 400 && 404 RESPONSES
cat status_new_endpoints.txt | grep "http" | grep -E "C=400  |C=404  " -v | grep -v "Pycurl" | cut -d "\"" -f2 > filtered_new_endpoints.txt

### MERGE ffuf.txt WITH NEW ENDPOINTS >> ffuf.txt
echo -e "\033[0;31m [+]\033[0m ADDING LIVE PATHS TO ffuf.txt"
cat filtered_new_endpoints.txt | anew ffuf.txt
rm filtered_new_endpoints.txt

### ADD http://$domain AND https://$domain EVEN IF THEY ARE 404/400/X status code >> ffuf.txt
echo "http://$domain\nhttps://$domain" | anew ffuf.txt

### PROXY ALL BRUTEFORCED DIRECTORIES TO BURP > status_dir.txt
echo -e "\033[0;31m [+]\033[0m PROXING ALL DIRECTORIES && FILES TO BURP SUITE"
wfuzz -f status_dir.txt,raw -L -Z -z file,ffuf.txt -z file,$HOME/tools/CRIMSON/words/blank -p 127.0.0.1:8080 -H $cookie FUZZFUZ2Z > /dev/null 2>&1

### CHECK FOR BACKUP FILES > backups.txt
echo -e "\033[0;31m [+]\033[0m CHECKING EXISTANCE OF BRUTEFORCED FILES BACKUPS"
python $HOME/tools/CRIMSON/scripts/crimson_backuper.py -w ffuf.txt -e $HOME/tools/CRIMSON/words/BCK_EXT -c $cookie | tee -a backups.txt

### EXTRACT UNIQUE QUERIES > exp/params.txt
echo -e "\033[0;31m [+]\033[0m PREPARING FILES FOR crimson_exploit MODULE"
cat status_params.txt | grep -v "C=404 " | grep http | grep -v "Pycurl" | cut -d "\"" -f2 | sort -u | qsreplace -a > exp/params.txt

### PREAPRE DIRECTORIES FOR EXPLOIT MODULE > exp/dirs.txt
cat ffuf.txt | grep -v -e "\.png\|\.js\|\.gif\|\.jpg\|\.png\|\.css" | grep -v "\/$" > temp_without_slash
cat temp_without_slash | sed "s/$/\//" > temp_with_slash
cat ffuf.txt | grep "\/$" >> temp_with_slash
sort -u temp_with_slash > exp/dirs.txt
rm temp_with_slash temp_without_slash

### CORS MISCONFIGURATION SCAN >> recon.txt
echo "[CORS] ---------------------------------------------------------" | tee -a recon.txt
cat ffuf.txt | CorsMe -t 70 -header $cookie -output corsme.txt > /dev/null 2>&1
cat corsme.txt >> recon.txt
rm corsme.txt
rm error_requests.txt

### DIG SECRETS FROM all_source_code/ SAVE COLORED OUTPUT > apikeys.txt
echo -e "\033[0;31m [+]\033[0m CHECKING SECRETS IN GATHERED SOURCE CODE"
grep -EHirn "accesskey|admin|aes|api_key|apikey|checkClientTrusted|crypt|password|pinning|secret|SHA256|SharedPreferences|superuser|token|X509TrustManager" all_source_code/ --color=always > apikeys.txt

### GET PARAMETER NAMES BY BRUTEFORCING THEM > exp/arjun.txt
echo -e "\033[0;31m [+]\033[0m STARTING ARJUN ON ffuf.txt - IT WILL TAKE A WHILE..."
arjun -i ffuf.txt -oT exp/arjun.txt -q --headers $cookie 

fi
